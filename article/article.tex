% !TEX root = ./article.tex

\documentclass{article}

\usepackage{mystyle}
\usepackage{myvars}



%-----------------------------

\begin{document}

	\maketitle % Insert title

	\thispagestyle{fancy} % All pages have headers and footers


%-----------------------------
%	ABSTRACT
%-----------------------------

	\begin{abstract}
		\noindent En este documento se realiza una descripción acerca de los algoritmos de clasificación basados en generación de \emph{Redes Bayesianas} y el \emph{Teorema de Bayes} referido a la probabilidad condicionada. Además, se realizan varios experimentos utilizando las estrategias de construcción de estructura de redes bayesianas descritas previamente, sobre conjuntos de datos de distintos tamaños mediante tests de \emph{Validación Cruzada} y un conjunto de datos formado por instancias independientes del resto.
	\end{abstract}

%-----------------------------
%	TEXT
%-----------------------------

	\section{Redes Bayesianas}
	\label{sec:bayes_network}

		\paragraph{}
		\say{Una red bayesiana es un modelo gráfico probabilístico que representa un conjunto de variables aleatorias y sus dependencias condicionales a través de un grafo acíclico dirigido. Por ejemplo, una red bayesiana puede representar las relaciones probabilísticas entre enfermedades y síntomas. Dados los síntomas, la red puede ser usada para computar la probabilidad de la presencia de varias enfermedades.} \cite{wiki:Bayesian_network}

		\paragraph{}
		\say{Formalmente, las redes bayesianas son grafos dirigidos acíclicos cuyos nodos representan variables aleatorias en el sentido de Bayes: las mismas pueden ser cantidades observables, variables latentes, parámetros desconocidos o hipótesis. Las aristas representan dependencias condicionales; los nodos que no se encuentran conectados representan variables las cuales son condicionalmente independientes de las otras. Cada nodo tiene asociado una función de probabilidad que toma como entrada un conjunto particular de valores de las variables padres del nodo y devuelve la probabilidad de la variable representada por el nodo.} \cite{wiki:Bayesian_network}

		\paragraph{}
		Existen algoritmos eficientes que llevan a cabo la inferencia y el aprendizaje en redes bayesianas. En este documento se habla de 3 de los algoritmos de generación de estructuras de red: \emph{Naive Bayes} en la sección \ref{sec:structure_naive}, \emph{K2} en la sección \ref{sec:structure_K2} y \emph{TAN} en la sección \ref{sec:structure_tan}.

		\subsection{Estructura Naive Bayes}
		\label{sec:structure_naive}

			\paragraph{}
			El algoritmo \emph{Naive Bayes} genera la estructura de red suponiendo que la clase depende de todos los nodos de manera simple, por lo que se crea un árbol de altura 1 donde el padre es la variable aleatoria que representa la clase mientras que los hijos están formados por los nodos que representan las variables aleatorias de los atributos del conjunto de datos.

		\subsection{Estructura K2}
		\label{sec:structure_K2}

			\paragraph{}
			El algoritmo de generación de redes \emph{K2} se basa en la suposición de que existe un orden total entre el conjunto de atributos y la clase del conjunto de datos. Dicha ordenación se realiza a partir del estimador de máxima similitud con corrección bayesiana, que se diferencia del de Laplace en que este varía para cada nodo. La estructura de la red se crea suponiendo que para el nodo $X_k$, puede tener como padres al subconjunto de nodos $X_1,...,X_{k-1}$ en el caso de que este mejore su puntuación.

		\subsection{Estructura TAN}
		\label{sec:structure_tan}

			\paragraph{}
			El algoritmo \emph{TAN} (o \emph{Tree Aumented NaiveBayes}) genera la estructura de red en dos fases. La primera de ellas se corresponde con el algoritmo \emph{Naive Bayes} descrito anteriormente en la sección \ref{sec:structure_naive}. La segunda fase compone las dependencias entre atributos (no utiliza el nodo de la clase) mediante el algoritmo de \emph{Chow y Liu}, que genera el \emph{árbol recubridor máximo} a partir de la ecuación \eqref{eq:mutual_information} (información mútua entre dos variables aleatorias condicionadas a la clase).

			\begin{equation}
			\label{eq:mutual_information}
				I_{\hat{Pr}}(X, Y | C) = \sum_{x,y} \hat{Pr}(x,y | C) log(\frac{\hat{Pr}(x,y | C)}{\hat{Pr}(x| C)\hat{Pr}(y | C)})
			\end{equation}

	\section{Experimentos}
	\label{sec:experiments}

		\paragraph{}
		Tras haber descrito las \emph{Redes Bayesianas} en la sección anterior, a continuación se presentan los resultados obtenidos tras realizar un conjunto de experimentos sobre el conjunto de datos \textbf{Credit}, que se describen en la sección \ref{data_set}. Los tests han consistido en la evaluación del comportamiento de los algoritmos de generación de \emph{Redes Bayesianas} implementadas en \emph{Weka} \cite{tool:weka} variando el tamaño de los conjuntos de entrenamiento.

		\paragraph{}
		La metodología seguida ha sido la siguiente: Para cada conjunto de datos de entrenamiento se ha realizado un experimento de \emph{Validación Cruzada de 10 particiones} a partir del cual se ha almacenado la tasa de error obtenida. Además, se han guardado los modelos generados en cada caso para después probarlos sobre un conjunto de datos independiente del de entrenamiento. Esta tarea se ha repetido con tres los conjuntos de datos de 3 tamaños diferentes sobre los algoritmos de generación de \emph{Redes Bayesianas} descritos en la sección anterior: \emph{Naive Bayes}, \emph{K2} y \emph{TAN}.

		\paragraph{}
		A continuación se describe la naturaleza del conjunto de datos utilizado para los experimentos así como los tamaños de sus particiones.

		\subsection{Conjunto de Datos}
		\label{sec:data_set}

			\paragraph{}
			El conjunto de datos utilizado se denomina \emph{Credit} y se puede acceder a él a través de \url{https://github.com/garciparedes/machine-learning-bayesian-2/tree/master/weka/datasets} \cite{garciparedes:machine-learning-bayesian-2}. Está formado por \textbf{11 atributos} de carácter nominal más la clase de destino formada por \textbf{2 valores}. Dichos atributos presentan un rango de valores reducidos, encontrandose todos ellos entre \textbf{2 y 4 valores distintos}.

			\paragraph{}
			En cuanto a los conjuntos de datos utilizados para los experimentos, todos ellos contienen los mismos atributos así como instancias representativas para todos los valores posibles de los atributos. Tampoco se dan atributos desconocidos. a continuación se describen los tamaños de cada conjunto de datos:

			\begin{itemize}
				\item \textbf{Datos\_Credit\_100}: Está formado por $100$ instancias.
				\item \textbf{Datos\_Credit\_1000}: Está formado por $1000$ instancias.
				\item \textbf{Datos\_Credit\_10000}: Está formado por $10000$ instancias.
				\item \textbf{Test\_Credit\_1000}: Está formado por $1000$ instancias.
			\end{itemize}

			\paragraph{}
			Por último es necesario describir tanto el estimador como los parámetros de configuración utilizados para cada algoritmo. En cuanto al \emph{método de estimación} de probabilidades, se ha escogido el \emph{Estimador de Máxima Verosimilitud con corrección de Laplace} y $m = 0.5$. Dicho estimador se muestra en la ecuación \eqref{eq:naive_bayes_smoothing}, donde $n_c$ representa el número de ejemplos de entrenamiento con la clase $b_j$, $n$ el número de ejemplos de la de entrenamiento con la clase $b_j$ y el atributo $a_i$. El valor $p$ se obtiene mediante $p = Pr(A = a_i | B = b_j)$, es decir, la estimación a priori y $m$ un determinado peso para la estimación a priori.

			\begin{equation}
			\label{eq:naive_bayes_smoothing}
			 Pr'(A = a_i | B = b_j) = \frac{n_c + mp}{n + m}
			\end{equation}

			\paragraph{}
			En cuanto a los algoritmos de clasificación, \emph{Naive Bayes} no tiene más parámetros adicionales. \emph{K2} se ha configurado para que no comience de una red \emph{Naive Bayes} y se ha permitido que cada nodo tenga un número arbitrario de padres. En el caso de \emph{TAN}, se ha seleccionado el método de puntuación de \emph{Máxima Verosimilitud con corrección de Laplace} descrito en la ecuación \eqref{eq:naive_bayes_smoothing} con parámetro $m = 0.5$ al igual que en el caso del método de estimación de probabilidades.

		\paragraph{}
		En la sección \ref{sec:results} se presentan los resultados obtenidos así como una discusión acerca de los mismos.

		\subsection{Resultados}
		\label{sec:results}

			\paragraph{}
			En la figura \ref{fig:bayes_network} se muestra la estructura de las distintas \emph{Redes Bayesianas} generadas por cada algoritmos dependiendo del tamaño del conjunto de datos de entrenamiento. Nótese que estas redes han sido generadas a partir de un experimento de \emph{Validación Cruzada de 10 particiones} sobre conjuntos de datos de \emph{100}, \emph{1000} y \emph{10000} instancias respectivamente.

			\paragraph{}
			En cuanto al algoritmo \emph{Naive Bayes}, la estructura de la red generada es trivial en ambos casos, suponiendo dependencia directa de la clase sobre todos los atributos, por lo que se genera un árbol de profundidad $1$ donde el nodo raiz es la clase y los nodos hoja se corresponden con los $11$ atributos. Nótese que el tamaño del conjunto de datos de entrenamiento no genera variaciones tal y como se muestra en las figuras \ref{fig:naive_100}, \ref{fig:naive_1000} y \ref{fig:naive_10000}.

			\paragraph{}
			Las redes generadas por el algoritmo \emph{K2} presentan una característica diferenciadora respecto del resto. En este caso no todos los atributos tienen como padre al nodo referido a la \emph{clase} (algunos a pesar de no ser la clase de destino no tienen padre). Esto se traduce de manera práctica en que dichos atributos no serán utilizados durante las fases de clasificación de nuevas instancias. La razón es la estrategia de construcción de redes seguida por este algoritmo, que añade una arista tan solo cuando la puntuación obtenida mediante el indicador de verosimilitud se mejora. Las estructuras generadas a partir de cada conjunto de datos de distinto tamaño se muestran en las figuras \ref{fig:k2_100}, \ref{fig:k2_1000} y \ref{fig:k2_10000}.

			\paragraph{}
			El algoritmo \emph{TAN} tal y como se ha descrito anteriormente, es una combinación del algoritmo \emph{Naive Bayes} y el algoritmo \emph{Chow Liu} obviando la clase de destino. Por tanto, debido a la primera fase de \emph{Naive Bayes}, la clase depende de todos los atributos directamente. En la fase de \emph{Chow Liu} se forman las relaciones entre atributos. Nótese que las estructuras de red formadas por \emph{TAN} tienen muchas más aristas que el resto, lo cual añade complejidad en el cálculo de probabilidades para la clasificación de nuevas instancias. Las redes generadas se muestran en las figuras  \ref{fig:tan_100}, \ref{fig:tan_1000} y \ref{fig:tan_10000}.

			\begin{figure}[h]
				\centering
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-simple-100}
					\caption{Naive Bayes - 100}
					\label{fig:naive_100}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-simple-1000}
					\caption{Naive Bayes - 1000}
					\label{fig:naive_1000}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-simple-10000}
					\caption{Naive Bayes - 10000}
					\label{fig:naive_10000}
				\end{subfigure} \\
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-k2-100}
					\caption{K2 - 100}
					\label{fig:k2_100}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-k2-1000}
					\caption{K2 - 1000}
					\label{fig:k2_1000}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-k2-1000}
					\caption{K2 - 10000}
					\label{fig:k2_10000}
				\end{subfigure} \\
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-tan-100}
					\caption{TAN - 100}
					\label{fig:tan_100}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-tan-1000}
					\caption{TAN - 1000}
					\label{fig:tan_1000}
				\end{subfigure} \
				\begin{subfigure}{.32\textwidth}
					\centering
					\includegraphics[width=\linewidth]{bayesnet-tan-10000}
					\caption{TAN - 10000}
					\label{fig:tan_10000}
				\end{subfigure}
				\caption{Redes Bayesianas generadas a partir de DatosCredit}
				\label{fig:bayes_network}
			\end{figure}


			\paragraph{}
			A continuación se describen los resultados a nivel de tasas de error obtenidas en los experimentos. Estos han sido resumidos en la tabla \ref{table:error_rates}. Tal y como se dijo anteriormente, los resultados obtenidos en la fase de entramiento se corresponden con una metodología de \emph{Validación Cruzada de 10 particiones}, mientras que los obtenidos en la fase de \emph{Test} son relativos a la clasificación de las instancias del conjunto de datos de $1000$ instancias a partir del modelo obtenido en la fase anterior.

			\paragraph{}
			La tendencia habitual de los tres algoritmos de clasificación es de reducir su tasa de error conforme el tamaño del conjunto de datos de entrenamiento aumenta. Sin embargo, se pueden apreciar dos casos atípicos (entrenamiento con \emph{K2}). Estos resultados pueden ser debidos a distintos factores como la generación puntual de particionamientos que hagan caer en el peor caso posible.

			\paragraph{}
			A nivel de resultados, los tres algoritmos ofrecen tasas de error similares. Debido a las pequeñas variaciones a nivel de tasas de error entre ellos, no se puede asumir que ninguno ofrezca mejores resultados que el resto sobre los conjunto de datos utilizados.

			\begin{table}
			\centering
			\small
			\begin{tabu}{ | c | c | c | c | c | c | c | c | c | c | }
				\hline
				\multicolumn{10}{ | c | }{Clasificación mediante Redes Bayesianas} \\ \hline
				\multirow{3}{*}{Datos}& \multicolumn{9}{ c |}{Tasa de Error} \\ \cline{2-10}
															& \multicolumn{3}{ c |}{Naive Bayes} & \multicolumn{3}{ c |}{K2} & \multicolumn{3}{ c |}{TAN}\\ \cline{2-10}
															& \emph{100} & \emph{1000} & \emph{10000} & \emph{100} & \emph{1000} & \emph{10000} & \emph{100} & \emph{1000} & \emph{10000}\\ \hline
				Entrenamiento		& $36.00\%$	 & $31.70\%$ & $29.69\%$ & $28.00\%$	 & $36.10\%$ & $28.50\%$	& $35.00\%$ & $29.50\%$ & $28.27\%$	\\ \hline
				Datos Test			& $30.00\%$	 & $27.80\%$ & $27.50\%$ & $33.30\%$	 & $30.40\%$ & $25.40\%$	& $33.30\%$ & $24.90\%$ & $25.00\%$	\\
				\hline
			\end{tabu}
			\caption{Tasas de error obtenida a partir de distintas configuraciones a nivel de estructura de \emph{Redes Bayesianas}}
			\label{table:error_rates}
		\end{table}


%-----------------------------
%	Bibliographic references
%-----------------------------
	\nocite{garciparedes:machine-learning-bayesian-2}
	\nocite{subject:taa}
	\nocite{tool:weka}
  \bibliographystyle{alpha}
  \bibliography{bib/misc}

\end{document}
